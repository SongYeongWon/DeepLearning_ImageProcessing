# LAB: Parking Management System

**Date:** 2022/05/26

**Author:** Song Yeong Won

**Github:** repository link

**Demo Video:** [Youtube link](https://youtu.be/fiFCeIMqOvg)



# Introduction

This tutorial is about create a simple program that counts the number of vehicles in the parking lot and display the number of available parking space using a CNN based object detection model. 

For the given dataset, the maximum available parking space is 13. If the current number of vehicles is more than 13, then, the available space should display as ‘0’.

# Requirement

## Hardware

- NVDIA GeForce GTX 1660 Ti

## Software Installation

- CUDA 11.4

  ```python
  # Check your CUDA version
  > nvidia-smi
  ```

  check your cuda version and donwload nvidia driver [click here]([CUDA Toolkit Archive | NVIDIA Developer](https://developer.nvidia.com/cuda-toolkit-archive))

- cudatoolkit 10.2
- Python 3.9.12
- Pytorch 1.9.1
- Torchvision==0.10.1
- YOLOv5

### Follow the steps

```python
# create a conda env name=py39
conda create -n py39 python=3.9.12
conda activate py39
conda install -c anaconda seaborn jupyter
pip install opencv-python


# pytorch with GPU
conda install -c anaconda cudatoolkit==10.2.89 cudnn seaborn jupyter
conda install pytorch==1.9.1 torchvision==0.10.1 torchaudio==0.9.1 cudatoolkit=10.2 -c pytorch
pip install opencv-python torchsummary


# Check GPU in Pytorch
conda activate py39
python
import torch
print("cuda" if torch.cuda.is_available() else "cpu")
```



# Dataset

Description of dataset goes here

This dataset is an video of the same parking lot at the same location. The video can be downloaded through the link below.

**Dataset link:** [Click here](https://drive.google.com/file/d/1d5RATQdvzRneSxvT1plXxgZI13-334Lt/view?usp=sharing)

# Tutorial Procedure

## Pretrained Checkpoints

The model should be selected in consideration of the accuracy and processing speed suitable for the purpose.

This project used the YOLOv5x model.

| Model                                                        | size (pixels) | mAPval 0.5:0.95 | mAPval 0.5 | Speed CPU b1 (ms) | Speed V100 b1 (ms) | Speed V100 b32 (ms) | params (M) | FLOPs [@640](https://github.com/640) (B) |
| ------------------------------------------------------------ | ------------- | --------------- | ---------- | ----------------- | ------------------ | ------------------- | ---------- | ---------------------------------------- |
| [YOLOv5n](https://github.com/ultralytics/yolov5/releases)    | 640           | 28.0            | 45.7       | **45**            | **6.3**            | **0.6**             | **1.9**    | **4.5**                                  |
| [YOLOv5s](https://github.com/ultralytics/yolov5/releases)    | 640           | 37.4            | 56.8       | 98                | 6.4                | 0.9                 | 7.2        | 16.5                                     |
| [YOLOv5m](https://github.com/ultralytics/yolov5/releases)    | 640           | 45.4            | 64.1       | 224               | 8.2                | 1.7                 | 21.2       | 49.0                                     |
| [YOLOv5l](https://github.com/ultralytics/yolov5/releases)    | 640           | 49.0            | 67.3       | 430               | 10.1               | 2.7                 | 46.5       | 109.1                                    |
| [YOLOv5x](https://github.com/ultralytics/yolov5/releases)    | 640           | 50.7            | 68.9       | 766               | 12.1               | 4.8                 | 86.7       | 205.7                                    |
| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases)   | 1280          | 36.0            | 54.4       | 153               | 8.1                | 2.1                 | 3.2        | 4.6                                      |
| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases)   | 1280          | 44.8            | 63.7       | 385               | 8.2                | 3.6                 | 12.6       | 16.8                                     |
| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases)   | 1280          | 51.3            | 69.3       | 887               | 11.1               | 6.8                 | 35.7       | 50.0                                     |
| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases)   | 1280          | 53.7            | 71.3       | 1784              | 15.8               | 10.5                | 76.8       | 111.4                                    |
| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases) + [TTA](https://github.com/ultralytics/yolov5/issues/303) | 1280 1536     | 55.0 55.8       | 72.7 72.7  | 3136 -            | 26.2 -             | 19.4 -              | 140.7 -    | 209.8                                    |

Further more information : [Click here]([Releases · ultralytics/yolov5 · GitHub](https://github.com/ultralytics/yolov5/releases))

## Import libraries

```python
import torch
import cv2
import random
from PIL import Image
```

## Setup

```python
# Load the Model
model = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True)
```

## Customized detect.py

```python
# YOLOv5 🚀 by Ultralytics, GPL-3.0 license
"""
Run inference on images, videos, directories, streams, etc.

Usage - sources:
    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam
                                                             img.jpg        # image
                                                             vid.mp4        # video
                                                             path/          # directory
                                                             path/*.jpg     # glob
                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube
                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch
                                         yolov5s.torchscript        # TorchScript
                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                         yolov5s.xml                # OpenVINO
                                         yolov5s.engine             # TensorRT
                                         yolov5s.mlmodel            # CoreML (macOS-only)
                                         yolov5s_saved_model        # TensorFlow SavedModel
                                         yolov5s.pb                 # TensorFlow GraphDef
                                         yolov5s.tflite             # TensorFlow Lite
                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
"""

import argparse
import os
import sys
from pathlib import Path
from cv2 import sqrt

import torch
import torch.backends.cudnn as cudnn
import math

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

from models.common import DetectMultiBackend
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams
from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,
                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import select_device, time_sync


import shutil
import os
#--------Color------------
Color_RED = (0,0,255)
Color_GREEN = (0,255,0)
Color_state = (0,0,0)
Spot_list = [110,210,310,410,490,580,680,770,860,940,1040,1130,1230]

path = "./runs/datalog"
file_list = os.listdir(path)
file_list_log = [file for file in file_list if file.startswith("logfile")]
count = 1+len(file_list_log)

@torch.no_grad()
def run(
        weights=ROOT / 'yolov5s.pt',  # model.pt path(s)
        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam
        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path
        imgsz=(640, 640),  # inference size (height, width)
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_txt=False,  # save results to *.txt
        save_conf=False,  # save confidences in --save-txt labels
        save_crop=False,  # save cropped prediction boxes
        nosave=False,  # do not save images/videos
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        update=False,  # update all models
        project=ROOT / 'runs/detect',  # save results to project/name
        name='exp',  # save results to project/name
        exist_ok=False,  # existing project/name ok, do not increment
        line_thickness=3,  # bounding box thickness (pixels)
        hide_labels=False,  # hide labels
        hide_conf=False,  # hide confidences
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
):
    source = str(source)
    save_img = not nosave and not source.endswith('.txt')  # save inference images
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))
    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)
    if is_url and is_file:
        source = check_file(source)  # download

    # Directories
    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir

    # Load model
    device = select_device(device)
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    imgsz = check_img_size(imgsz, s=stride)  # check image size

    # Dataloader
    if webcam:
        view_img = check_imshow()
        cudnn.benchmark = True  # set True to speed up constant image size inference
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)
        bs = len(dataset)  # batch_size
    else:
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)
        bs = 1  # batch_size
    vid_path, vid_writer = [None] * bs, [None] * bs

    # Run inference
    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup
    dt, seen = [0.0, 0.0, 0.0], 0
    with open("runs/datalog/logfile.txt", "w") as f:
        for path, im, im0s, vid_cap, s in dataset:
            t1 = time_sync()
            im = torch.from_numpy(im).to(device)
            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim
            t2 = time_sync()
            dt[0] += t2 - t1

            # Inference
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            pred = model(im, augment=augment, visualize=visualize)
            t3 = time_sync()
            dt[1] += t3 - t2

            # NMS
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
            dt[2] += time_sync() - t3

            # Second-stage classifier (optional)
            # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)
            font =  cv2.FONT_HERSHEY_PLAIN
            center_list = [x for x in range(90,1280,100)]

            center_spot =[0,0,0,0,0,0,0,0,0,0,0,0,0] #각 자리에 있는지 없는지 0 or 1
            park_num = 0
            empty_num = 0
            
            for i, det in enumerate(pred):  # per image
                seen += 1
                if webcam:  # batch_size >= 1
                    p, im0, frame = path[i], im0s[i].copy(), dataset.count
                    s += f'{i}: '
                else:
                    p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)

                f.write(f"frame number = {frame}")
                count_spot = [[],[],[],[],[],[],[],[],[],[],[],[],[]] #각 자리에 있는 box index 저장

                p = Path(p)  # to Path
                save_path = str(save_dir / p.name)  # im.jpg
                txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt
                s += '%gx%g ' % im.shape[2:]  # print string
                gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
                imc = im0.copy() if save_crop else im0  # for save_crop
                annotator = Annotator(im0, line_width=line_thickness, example=str(names))
                if len(det):
                    # Rescale boxes from img_size to im0 size
                    det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()

                    # Print results
                    for c in det[:, -1].unique():
                        n = (det[:, -1] == c).sum()  # detections per class
                        s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string

                    obj_num = 1
                    center_list = [90,190,290,390,490,590,690,790,890,990,1090,1190,1250]
                    # Write results
                    conf_spot = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]] 
                    for *xyxy, conf, cls in reversed(det):
                        Class = int(cls)
                        if save_txt:  # Write to file
                            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                            line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                            with open(f'{txt_path}.txt', 'a') as f:
                                f.write(('%g ' * len(line)).rstrip() % line + '\n')
                        
                        if (Class != 2) and (Class != 5) and (Class != 7):
                            continue

                        conf_spot[obj_num-1].append(float(conf)) 
                        center_x = 0
                        center_y = 0
                        if xyxy[3] <440:
                            center_x = int((xyxy[0]+xyxy[2])/2)
                            center_y = int((xyxy[1]+xyxy[3])/2)
                        
                        for j,n in enumerate(center_list):
                            length = math.sqrt(math.pow(center_x-n, 2) + math.pow(center_y-375, 2))
                            if int(length) <= 55:
                                count_spot[j].append(obj_num)
                                if center_spot[j] == 0:
                                    center_spot[j] = 1
                        obj_num +=1 

                    max_obj = 0
                    for k in range(13):
                        if len(count_spot[k]) == 0:
                            center_spot[k] = 0
                        if len(count_spot[k]) == 2:
                            if conf_spot[count_spot[k][0]] > conf_spot[count_spot[k][1]] :
                                max_obj = count_spot[k][0]
                            else :
                                max_obj = count_spot[k][1]
                            count_spot[k] = [max_obj]

                    for c in range(len(center_spot)):
                        cv2.line(im0, (90+100*c,  375),  (90+100*c,  375), (0,0,255), 5)
                        if c == 12:
                            cv2.line(im0, (1250,  375),  (1250,  375), (0,0,255), 5)
                        if center_spot[c] == 0:
                            Color_state = Color_GREEN
                            empty_num += 1
                        else :
                            Color_state = Color_RED
                            park_num += 1
                        cv2.putText(im0, str(c+1), (Spot_list[c], 230), font, 2, Color_state, 3, cv2.LINE_AA)

                    box_index = 1
                    for *xyxy, conf, cls in reversed(det):
                        f.write(f"class = {2}")
                        Class = int(cls)
                        if save_txt:  # Write to file
                            xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                            line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                            with open(f'{txt_path}.txt', 'a') as f:
                                f.write(('%g ' * len(line)).rstrip() % line + '\n')
                        
                        if (Class != 2) and (Class != 5) and (Class != 7):
                            continue
                        
                        if [box_index] in count_spot:
                        
                            if xyxy[3] <440:
                                center_x = int((xyxy[0]+xyxy[2])/2)
                                center_y = int((xyxy[1]+xyxy[3])/2) 
                                cv2.line(im0, (center_x, center_y), (center_x, center_y), (0,255,0), thickness=5)
                                if save_img or save_crop or view_img:  # Add bbox to image
                                    c = int(cls)  # integer class
                                    if c != 2:
                                        c = 2
                                    label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')
                                    annotator.box_label(xyxy, label, color=colors(c, True))
                                
                                if save_crop:
                                    save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)
                        box_index += 1 
                
                f.write(f"park space = {park_num}\n")

                cv2.putText(im0, "Frame  = ", (34, 700), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                cv2.putText(im0, str(frame), (200, 700), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                cv2.putText(im0, "park space = ", (34, 600), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                cv2.putText(im0, str(park_num), (350, 600), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                cv2.putText(im0, "empty space = ", (34, 500), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                cv2.putText(im0, str(empty_num), (350, 500), font, 2, (255, 255, 255), 3, cv2.LINE_AA)
                
                # Stream results
                im0 = annotator.result()
                if view_img:
                    cv2.imshow(str(p), im0)
                    cv2.waitKey(1)  # 1 millisecond

                # Save results (image with detections)
                if save_img:
                    if dataset.mode == 'image':
                        cv2.imwrite(save_path, im0)
                    else:  # 'video' or 'stream'
                        if vid_path[i] != save_path:  # new video
                            vid_path[i] = save_path
                            if isinstance(vid_writer[i], cv2.VideoWriter):
                                vid_writer[i].release()  # release previous video writer
                            if vid_cap:  # video
                                fps = vid_cap.get(cv2.CAP_PROP_FPS)
                                w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                                h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                            else:  # stream
                                fps, w, h = 30, im0.shape[1], im0.shape[0]
                            save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos
                            vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
                        vid_writer[i].write(im0)
            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')

            # Print time (inference-only)
        # LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')
    shutil.move('runs/datalog/logfile.txt', 'runs/datalog/logfile{0}.txt'.format(count))

    # Print results
    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image
    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)
    if save_txt or save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    if update:
        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)


def parse_opt():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')
    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')
    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')
    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')
    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')
    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='show results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--visualize', action='store_true', help='visualize features')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')
    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')
    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')
    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')
    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand
    print_args(vars(opt))
    return opt


def main(opt):
    check_requirements(exclude=('tensorboard', 'thop'))
    run(**vars(opt))


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)

```

## Validate pretrained model with YOLOv5x

```python
!python detect.py --weights yolov5x.pt --img 640 --conf 0.25 --source DLIP_parking_test_video.mp4
```



## Algorithm

1. Validation using pretrained YOLOv5x model.
2. Do not label objects other than cars.
3. Check whether the vehicle is parked by using the parking line threshold value.
4. Determining where the vehicle is located is identified by the distance between the center coordinates of the vehicle bounding box and the center coordinates of each parking spot.
5. Compare the confidence values to make a duplicate label a single label.
6. For each frame, indicate where the vehicle is located and where it is not, and the number of vehicles parked.

### Filtering Labels

In the this video, cars are labeled in three types "car", "truck", and "bus". It also determines all objects, people except cars. Therefore, unnecessary information except for "car", "truck", and "bus" was removed.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/before_filter.jpg" alt="img" style="zoom:67%;" /> |
| :----------------------------------------------------------: |
| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/after_filter.jpg" alt="img" style="zoom:67%;" /> |
|                 **Figure 1. label filtered**                 |

In the first image of Figure 1,all objects are labeled. After the label filter, the second image of Figure 1 is labeled only for the car.

### Parking Status

The main purpose of this project is to identify how many vehicles are parked and to provide the number of spaces currently available for parking. First of all, it was checked whether the car was parked regardless of the parked location. The parking status of the vehicle can be checked as follows. Testing the dataset video through the YOLOv5x model has a labeling and bounding box for all cars. At this time, values of xmin, ymin, xmax, ymax, confidence, and class may be checked for each bounding box. If the ymax coordinate value was smaller than the y value of the parking line, it was determined that the vehicle was added regardless of which parking space it was in. If the ymax coordinate value is greater than the y value of the parking line, it was determined that the vehicle was not completely parked. In addition, only the object determined that the vehicle is parked is indicated by the bounding box and labeling.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/raw_image.jpg" alt="img" style="zoom: 67%;" /> |
| :----------------------------------------------------------: |
| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/y_value_filter.jpg" alt="img" style="zoom:67%;" /> |
|            **Figure 2. checking parked vehicle **            |

Looking at the above picture in Figure 2, all objects are labeled regardless of whether they are parked or not. It can be seen that the figure below in Figure 2 is labeled only for vehicles parked based on the parking line.



### Location of the parked vehicle

If the vehicle is parked, it is necessary to identify where it is located. In the video, there are a total of 13 parking spots. The algorithm for determining where the vehicle is located is identified by the distance between the center coordinates of the vehicle bounding box and the center coordinates of each parking spot. If the center coordinate of the bounding box and the center coordinate distance of the parking lot are less than the threshold distance value, it can be confirmed that the vehicle is located at the location where the center coordinate is located. For 13 parking spaces, 13 flags were set, and each frame was recorded as 1 if there was a difference and 0 if there was no difference. Every frame checks for all objects, so the previous information is constantly updated.

```python
center_spot =[0,0,0,0,0,0,0,0,0,0,0,0,0]
#center_spot is initially predefined about 13 parking spot Flag to zero
```

```python
#line 204
for j,n in enumerate(center_list):
    length = math.sqrt(math.pow(center_x-n, 2) + math.pow(center_y-375, 2))
    if int(length) <= 55: #threshold length is 55
        count_spot[j].append(obj_num)
        if center_spot[j] == 0:
            center_spot[j] = 1
```

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/463frame.jpg" alt="img" style="zoom:67%;" /> |
| :----------------------------------------------------------: |
| ![img](https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/463frame_result.png) |
|    **Figure 3. Check vehicle location about Frame 463 **     |

The second output result of Figure 3 is the output of center_spot. Only the 12th digit is zero, indicating that the car is empty.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/917frame.jpg" alt="img" style="zoom:67%;" /> |
| :----------------------------------------------------------: |
| ![img](https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/917frame_result.png) |
|     **Figure 4. Check vehicle location about Frame 917**     |

The second output result of Figure 4 is the output of center_spot. Only the third, fifth, and 12th digits are zero, indicating that the car is empty.

### Remove Duplicated Labels

There are three types of cars labeled: "car", "truck", and "bus". In some cases, it is not exactly classified as a car, but as a car and a truck at the same time. Process to classify cases classified as overlapping into one result value is needed. Confidence values were used to remove overlapping labeling. If two bounding boxes exist for the same place, the confidence values of the labeling data are compared with each other. Use only object detection labeling with a higher confidence value. Since the order of object detection is random for each frame, the order of objects is assigned so that the object number and the confidence value of the corresponding object can be stored and compared.

```python
##line 197
conf_spot[obj_num-1].append(float(conf)) 
```

```python
#line 213
for k in range(13):
    if len(count_spot[k]) == 0:
        center_spot[k] = 0
    if len(count_spot[k]) == 2:
        if conf_spot[count_spot[k][0]] > conf_spot[count_spot[k][1]] :
            max_obj = count_spot[k][0]
        else :
            max_obj = count_spot[k][1]
            count_spot[k] = [max_obj]
```

The red number indicates the parking position and the blue number indicates the order of object numbering.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/33frame_bf.jpg" alt="img" style="zoom:67%;" /> |
| :----------------------------------------------------------: |
|        **Figure 5. Before remove duplicated Labels**         |

In Figure 5, object numbering sequences 1 and 2 are overlapped and labeled for one object.

| ![img](https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/33frame_result.png) |
| :----------------------------------------------------------: |
|     **Figure 6. process of removing duplicated Labels**      |

In Figure 6, conf_spot is a list that stores confidence corresponding to the object numbering sequence. count_spot indicates where the vehicle labeling exists. Confidence values of objects 1 and 2 in conf_spot correspond to 0.63135 and 0.64327. That is, the first and second confidence values of the conf_spot indicate the confidence of objects 1 and 2. revised_count_spot indicate the count_spot after removing duplicate labels. Among the confidence values of object 1 and object 2, it can be seen that only object 2 remains in the revised_count_spot.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/33frame_af.jpg" alt="img" style="zoom:67%;" /> |
| :----------------------------------------------------------: |
|         **Figure 7. After remov duplicated Labels**          |

In Figure 7 shows the results after removing duplicate labels. In Figure 5, both object 1 and 2 were labeled, but in In Figure 7, it can be seen that the duplicates were removed and only object 2 was labeled.

# Results and Analysis

The results are as follows. When the overlapping labeling is removed, only one bounding box exists in each parking space, and thus an accurate parking space may be identified. Each frame identifies the number of parked cars and stores them as a text file. As shown in the figure below, the space where the vehicle is parked can be checked through red letters and the empty space can be checked through green letters. Also, it can be checked through park space and empty space in the image.

| <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/1350frame.jpg" alt="img" style="zoom:67%;" /> | <img src="https://raw.githubusercontent.com/SongYeongWon/DeepLearning_ImageProcessing/main/LAB/LAB4_Image/1350frame_result.png" alt="img"  /> |
| ------------------------------------------------------------ | ------------------------------------------------------------ |

| **Figure 8. Final Result ** |
| :-------------------------: |



# Reference

YOLOv5 : [Click here]([ultralytics/yolov5: YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite (github.com)](https://github.com/ultralytics/yolov5))

Object Detection with RetinaNet : [Click here](https://keras.io/examples/vision/retinanet/#load-the-coco2017-dataset-using-tensorflow-datasets)



# Appendix

### video Demo Link

* Link (Parking Management System) : [Click here](https://youtu.be/fiFCeIMqOvg)
