# **LAB: Straight Lane Detection and Departure Warning**

Name : Song Yeong Won

Student number : 21700375

# I. Introduction

In this project, you are required to create a program that displays lane departure warning message from the road lane detection.

To simplify this lab, only the straight lanes are to be detected and other difficult driving environment such as curved lanes and road intersections are ignored.

You can use any algorithm you learnt in class. If you want to use any other algorithms which are not covered in the lecture, you should describe the principle of those algorithms fully in the report. The evaluation of the project will be based on the success detection rate of the lanes of the test and validation video clips.

You must explain the whole process with an algorithm flowchart. Show and explain selected example results for each image process.

# II. Image processing

## 2.1 Flowchart

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/LAB3.png" alt="img" style="zoom: 80%;" /> |
| :----------------------------------------------------------: |
|                   **Figure 1. Flowchart**                    |

## 2.2 Image preprocessing

A video is regarded as one image per frame, and image processing is required for each frame. Ultimately, an image preprocessing process is required for Straight Lane edge detection. Figure 2 is an image in which Gaussian filter is applied after converting the original frame image to gray scale. It was used for smoothing noise and images existing on the original frame.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Canny_edge.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: |
|             **Figure 3. Canny edge detection **              |

Figure 3 shows an image to which a Gaussian filter is applied and an edge is detected through the Canny edge method. Although the edge of the lane is detected, there is a problem that other objects and backgrounds are also detected. Therefore, additional process is needed to detect only the desired part using ROI techniques.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/ROI.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: |
|                  **Figure 4. Area of ROI**                   |

Figure 4 is an image representing the ROI region. Since the original frame image itself is biased to the right, the roi area for the left is set wider. The ROI area was used in the form of a trapezoid rather than a square. In the case of a square, it is difficult to accurately detect only lanes because the edges of cars existing in other lanes are detected together. Therefore, ROI was set in the form of a trapezoidal shape to detect only the area corresponding to the lane.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/canny_ROI.png" alt="img" style="zoom:33%;" /> | <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/ROI_line.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|              **Figure 5.(a) Area of Canny ROI**              |               **Figure 5.(b) HoughLineP line**               |

Figure 5(a) is an image of a Canny Edge image detected through ROI. Previously, in Figure 3, other object edges were detected in addition to the line, but it can be confirmed that only the portion of the lane area was detected using the ROI area. Figure 5(b) is an image in which several lines are detected using the HoughLineP method. It is necessary to make several lines existing in the right and left lines into one. In the process of using HoughLineP, it is important to set several parameters such as Threshold, minLineLength, and maxLineGap. This part will be additionally mentioned in the discussion part. In addition, the process of making it into a single line will be explained in detail in Section 3.3 Line classification process.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/frame_result.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: |
|            **Figure 6. Result of Lane Detection**            |

Therefore, it appears in Figure 6 when the right and left lines are made into only one line. One line exists on the left and right sides, respectively, and Lane area is created at the intersection of the lines.

## 2.3 Line classification

In some cases, several lines exist in the image extracted through the HoughLineP method, or lines are not detected. First, consider the case where line is detected. We should detect only one line for each of the right and left lanes. Therefore, it is necessary to make several lines into only one line. So, the right and left sides were classified according to the slope value. For the right line, a line with one slope and y-intercept value was extracted using the average value of the slope of all straight lines and the average value of the y-intercept. The same process was performed for the left line. The equation for extracting one line is as follows. Cross represents the intersection of two straight lines.

$$
left_Y : average\,y\,intercept 
\\
left_T : average\,slope
\\
left_X = (720-left_Y)/left_T
\\
right_X = (720-right_Y)/right_T
\\
cross_X = (right_Y-left_Y)/(left_T-right_T)
\\
cross_X = left_T*cross_X + left_Y
$$
There are four cases of Line. The first case is straight, the second case is lane change to the right, the third case is lane change to the left, and the last case is lane change is when the lane is located in the center of the frame. First, the case of going straight, changing lanes to the right, and changing lanes to the left was classified according to the degree of Bias. Threshold_bias value has a positive value when changing the right lane and a negative value when changing the left lane. Thus, (positive)Threshold_bias value was specified to change the lane to the right if it was greater than this value, (negative)Threshold_bias value to the left if it was smaller than the (absolute)Threshold_bias value, and cases were classified straight. However, if a lane is changed and the lane is located in the center of the frame, it cannot be identified only by the Threshold_bias value. Therefore, in this case, the case was classified as the distance value between the lane and the frame center point. In the fourth case, when only the right line is detected, when only the left line is detected, there is a case where both lines are detected. If only one line is detected during the lane change process, only the corresponding line was output. If both lines are detected, the line connecting the intersection of the two lines and the center point of the two lines was extracted and output as one line. In this way, special cases were classified so that lane changes were made more naturally in the process of lane changes.

The fourth case will be described in detail below. In this case, the absolute value of bias is very low. Previously, the case of Line was classified according to the bias value, and if the bias absolute value is low, it is a Line change situation but is considered straight. Therefore, additional case distinctions are needed for this case. As the classification condition, the distance value between each line and the center of the frame was used as the threshold value. Since the threshold value is positive for the right line and negative for the left line, if the absolute value of the threshold value is less than 230, it is classified as the fourth case. In the fourth case, the results for each of the three cases are as follows.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Left_detect.png" alt="img" style="zoom:33%;" /> | <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Left_detect_direction.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|           **Figure 7(a). Olny Left Lane detected**           |         **Figure 7(b). Result of Left Lane change**          |

Figure 7(a) is a frame for switching to the left. In this case, only the left line with a negative slope was detected. Therefore, the line is detected only for the left line and expressed as one line. Looking at Figure 7(b), since the line is currently being changed to the left, the left arrow and area are shown in red.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Both_detect.png" alt="img" style="zoom:33%;" /> | <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Both_detectleft.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|      **Figure 8(a). Both Left and Right Lane detected**      |        **Figure 8(b). Lane is centered on the frame**        |

Figure 8(a) is an image in which a lane is located in the center of a frame during a direction change to the left. This case is when both right and left lines are detected for the center lane. In this case, a line connecting the intersection point where the two straight lines meet and the center point of the x-coordinate of the two straight lines was detected and used. Therefore, the process of changing the central lane was made more natural. Referring to Figure 8(b), it may be seen that a line is formed in the center of the lane. The arrow indicates a left direction because it is still moving the lane to the left.

| <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Right_detect.png" alt="img" style="zoom:33%;" /> | <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Right_direction.png" alt="img" style="zoom:33%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|          **Figure 9(a). Olny Right Lane detected**           |         **Figure 9(a). Result of Left Lane change**          |

Figure 9(a) is a frame for turning to the left. In this case, only the right line with a positive slope was detected. Referring to Figure 9(b), it may be seen that a line is detected only for the right line and is expressed as one line.

# III. Discussion

* Line detection Error Check

  In the process of detecting the line, there are cases in which only one line is not detected or both lines are not detected. If the line does not exist, the previous line was used. However, if the phenomenon that the line is not detected continues, continuing to use the previous line is using uncertain information. Therefore, a process of error checking was added to detect such an error. If more than 5 lines are detected per 20 frames, the previous line value was used as it is. However, if the line detection rate is less than 25%, the Dangerous sign is displayed on the screen, and the Lane detection area is displayed in red to indicate an error. If a continuous error is detected, it can be determined that it is driving on an unsafe and dangerous road.

  | <img src="https://raw.githubusercontent.com/wonhg1446/DLIP_LAB/main/Image/LAB3_Image/Error_checking.png" alt="img" style="zoom:33%;" /> |
  | :----------------------------------------------------------: |
  |               **Figure 10. Lane Error check**                |

  Figure 10 is the frame with error check. Previously, errors were indicated when detected below 25%, but in that case, lines were not detected below 25%. Figure x arbitrarily causes an error when the line detection rate is less than 75%.

  

* Line detection according to parameter setting of HoughLineP

  Setting parameters was important when using HoughLineP. Typical examples are Threshold, minLineLength, and maxLineGap. Threshold refers to a threshold that can be determined by a straight line. That is, the higher the threshold value, the more lines with more intersections are detected on the Hough space. Initially, in many cases, line was not detected because the threshold value was increased by 100 or more, and error checks occurred a lot. By adjusting the threshold value, it is possible to accurately detect the line without slowing down the calculation processing speed much. minLineLength means the minimum length of the line segment to be detected. By determining the minimum length of the line to be detected, it was possible to remove outlier segments that could cause problems with line detection. maxLineGap means the maximum edge point interval to be considered a straight line. If the interval is greater than or equal to the specified maxLineGap, it will be removed. Therefore, it was possible to regulate the length of the maximum line to be detected by adjusting the maxLineGap value. By adjusting the parameters in various ways, we tried to find the optimal parameter.

  

# IV. Conclusion

Through this lab, we were able to grasp the part of line detection along with the overall image processing process. Image preprocessing should be used well for the purpose. Since lane recognition is the main purpose of the experiment, it was important to extract only information about lanes using ROI. It is also important to learn the principles and usage of basic line detection methods such as Canny edge detection and HoughLineP. Since the use of filters, Canny edge detection, HoughLineP, and algorithms for recognizing lanes are all organic, building optimal algorithms was very important for processing speed.

# Appendix

### video Demo Link

* Link (Straight Line) : [Click here](https://youtu.be/yzTam5ttT_8)
* Link (Lane Change) : [Click here](https://youtu.be/UvrjnPqjqe8)

### source code

```python
"""## Import OpenCV Library"""
from audioop import bias
import math
from operator import imod
from turtle import right, st
import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
import timeit

cap = cv.VideoCapture("road_lanechange_student.mp4")

width = cap.get(cv.CAP_PROP_FRAME_WIDTH)
height = cap.get(cv.CAP_PROP_FRAME_HEIGHT)

#------------직선의 기울기, 좌표 변수-----------
m1 = 0          #매번 확인하는 기울기 값
y_intercept = 0 #직선의 y 절편 값

left_val =0     #왼쪽 line y절편 평균 값
left_avg = 0    #왼쪽 line 기울기 평균 값

right_val = 0   #오른쪽 line y절편 평균 값
right_avg = 0   #오른쪽 line 기울기 평균 값

x_left  = 0     #왼쪽 line x 좌표
x_right = 0     #오른쪽 line x 좌표

x_cross = 0     #교점 x 좌표
y_cross = 0     #교점 y 좌표
#------------이전 값 저장 변수들---------------
prev_left_y = 0     #왼쪽 line 이전 y절편 평균 값 저장
prev_left_m = 0     #왼쪽 line 이전 기울기 평균 값 저장

prev_right_y = 0    #오른쪽 line 이전 y절편 평균 값 저장
prev_right_m = 0    #오른쪽 line 이전 기울기 평균 값 저장

#-------------------색상 변수-----------------
left_color = (0,0,0)    #왼쪽 line 색상 초기화
right_color = (0,0,0)   #오른쪽 line 색상 초기화

line_detect = (255,0,0)         #Line detect 경우 Blue
line_non_detect = (0,255,255)   #Line non detect 경우 Yellow
line_change = (0,0,255)         #Line change 경우 Red

In_line_color = (0,255,0)       # 초기 Text green
Line_safe = (0,255,0)           # Safe 경우 green
Line_Dan = (0,0,255)            # Dangerous 경우 red

#-----------------Bias 관련 변수---------------
Bias = 0                #Bias initialization
Bias_Threshold = 20     #Bias Threshold
length_left = 0         #frame 중앙과 왼쪽 line 사이 거리
length_right = 0        #frame 중앙과 오른쪽 line 사이 거리
Threshold_length = 230  # Line과 frame 중앙사이 Threshol 거리 값
Thres_flag = 0          # 0 : 직진, 1 : 우측으로 차선변경, 2 : 좌측으로 차선변경, 3 : frame 중앙에 line 위치
Line_arrow = 0          # Line direction flag (0 : straight, 1 : right, 2 : left)
Line_df = 0

#------------Frame error check Variables-------
frame_num = 0;          #Count frame number 
L_frame = 0             #Left line detect check
R_frame = 0             #Right line detect check
frame_Theshold = 5      #Error Threshold value
No_line_sign = 0        #Frame error Flag ( 0 : safe, 1 : Dangerous)

font =  cv.FONT_HERSHEY_PLAIN

while(1):
    ret, frame = cap.read()
    if frame is None:
        break
    if ret is True: 
        # 알고리즘 시작 시점
        start_t = timeit.default_timer()    #frame 시작 시점 시간
        # start_t = cv.getTickCount()

    '''1. Image pre-processing'''
    #frame 마다 gray scale 로 변환
    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    #Guassian Filter 적용
    frame_gray = cv.GaussianBlur(frame_gray,(5,5),0)
    # Canny Edge Detection
    dst = cv.Canny(frame_gray, 50, 200, None, 3)

    '''Making ROI process'''
    mask = np.zeros_like(frame_gray)
    vertices = np.array([[100,630],[617,420],[685,420],[1000,630]], dtype=np.int32)
    cv.fillPoly(mask, [vertices], 255)
    #Canny Edege detection 한 이미지와 masking
    ROI_image = cv.bitwise_and(dst, mask)

    '''2. Line detection process'''
    #HoughLineP line 추출
    linesP = cv.HoughLinesP(ROI_image, 1, np.pi / 180 , 65, None, minLineLength=10, maxLineGap=50)
    cv.line(frame, (640,720), (640,630), (255,0,255), 3, cv.LINE_AA)

    #y 절편, 기울기 값 초기화 
    left_m = []
    left_y = []
    right_m = []
    right_y = []

    if linesP is not None:
        for i in range(0, len(linesP)):
            l = linesP[i][0]
            if(l[2]-l[0] != 0):
                m1 = (l[3]-l[1])/(l[2]-l[0])
                y_intercept =  l[1] - m1*l[0] 

            if(Thres_flag==0):   #직진의 경우 left, right line 구분 
                if(m1<-0.55):
                    left_m.append(m1)
                    left_y.append(y_intercept) 
                if(0.5<m1):
                    right_m.append(m1)
                    right_y.append(y_intercept)
            else :              #line 변경 경우 left, right line 구분 
                if(m1<-0.1):
                    left_m.append(m1)
                    left_y.append(y_intercept) 
                if(0<m1):
                    right_m.append(m1)
                    right_y.append(y_intercept)
        
    if( len(right_m) > 0):
        right_avg = sum(right_m)/len(right_m)   # right line y절편 평균값
        right_val = sum(right_y)/len(right_y)   # right line 기울기 평균값
        # right line의 이전 y절편, 기울기 평균 값 저장
        prev_right_y = right_val
        prev_right_m = right_avg
        # right line detection color(blue)
        right_color = line_detect
        # Detected right line count up
        R_frame += 1

    if( len(left_m) > 0):
        left_avg = sum(left_m)/len(left_m)  # left line y절편 평균값
        left_val = sum(left_y)/len(left_y)  # left line 기울기 평균값
        # left line의 이전 y절편, 기울기 평균 값 저장
        prev_left_y = left_val
        prev_left_m = left_avg
        # left line detection color(blue)
        left_color = line_detect
        # Detected left line count up
        L_frame += 1

    if( len(right_m) == 0):
        right_avg = prev_right_m    #이전 right line 의 기울기 값 사용
        right_val = prev_right_y    #이전 right line 의 y절편 값 사용
        # right line non detection color(green)
        right_color = line_non_detect

    if( len(left_m) == 0):
        left_avg = prev_left_m      #이전 left line 의 기울기 값 사용
        left_val = prev_left_y      #이전 left line 의 y절편 값 사용
        # left line non detection color(green)
        left_color = line_non_detect

    ''' Calculation of Line coordinate'''
    x_left = (720-left_val)/left_avg
    x_right = (720-right_val)/right_avg
    x_cross = (right_val-left_val)/(left_avg-right_avg)
    y_cross = left_avg*x_cross + left_val
    length_right = float(round(x_right-640))
    length_left = float(round(640- x_left))

    center = ( (int(x_right) + int(x_left) ) /2 )
    Bias =  float((640-int(center))  / (int(x_right) - int(x_left)) *100)

    img = cv.putText(frame, "Bias  = ", (34, 50), font, 2, (0, 0, 0), 1, cv.LINE_AA)
    if(Bias>0):
        img = cv.putText(frame, "Right", (200, 50), font, 2, (0, 0, 0), 1, cv.LINE_AA)
    elif(Bias<-0):
        img = cv.putText(frame, "Left", (200, 50), font, 2, (0, 0, 0), 1, cv.LINE_AA)
    '''3. Line detection Error check process'''
    #frame error check
    frame_num += 1
    if(frame_num == 20):
        if(L_frame <frame_Theshold and R_frame < frame_Theshold):
            No_line_sign = 1    #두 라인 모두 20 frame 중 5번 미만으로 detect 되는 경우(25%이하) 
        else:
            No_line_sign= 0
        #frame count value initialization
        frame_num = 0
        R_frame=0
        L_frame=0

    '''4. Line Case classification process'''
    #직선 경우 Bias Threshold 값에 따라 구분
    if(-1*Bias_Threshold<Bias<Bias_Threshold):
        Thres_flag = 0
    #우측으로 차선 변경 (Bias Threshold보다 클 경우)
    if(Bias_Threshold<Bias):
        Thres_flag = 1
    #좌측으로 차선 변경 (-Bias Threshold보다 작을 경우)
    if(Bias<-1*Bias_Threshold):
        Thres_flag = 2
    #우측 좌측 라인의 너비가 모두 Threshold_length 보다 작은 경우
    elif((length_right <Threshold_length) and (length_left <Threshold_length) ):
        Thres_flag = 3

    #Lane Class에 따라 line 그리기
    if(Thres_flag==0):
        cv.line(frame, (int(x_left),720), (int(x_cross),int(y_cross)), left_color, 3, cv.LINE_AA)
        cv.line(frame, (int(x_right),720), (int(x_cross),int(y_cross)), right_color, 3, cv.LINE_AA)
    if(Thres_flag == 1):
        cv.line(frame, (int(x_right),720), (int(x_cross),int(y_cross)), line_change, 3, cv.LINE_AA)
    if(Thres_flag == 2):
        cv.line(frame, (int(x_left),720), (int(x_cross),int(y_cross)), line_change, 3, cv.LINE_AA)     
    if(Thres_flag == 3):
        #오른쪽 라인만 detection 되는 경우
        if(len(right_m)>0 and len(left_m) == 0):
            cv.line(frame, (int(x_right),720), (int(x_cross),int(y_cross)), line_change, 3, cv.LINE_AA)
        #오른쪽, 왼쪽 라인 둘다 detection 되는 경우
        elif(len(right_m) > 0 and len(left_m) > 0):
            cv.line(frame, ( int(  (int(x_left) + int(x_right) )/2) ,720), (int(x_cross),int(y_cross)), line_change, 3, cv.LINE_AA)
        #왼쪽 라인만 detection 되는 경우
        else:
            cv.line(frame, (int(x_left),720), (int(x_cross),int(y_cross)), line_change, 3, cv.LINE_AA)       
    cv.line(frame, (int(x_cross),int(y_cross)), (int(x_cross),int(y_cross)), (0,255,255), 10, cv.LINE_AA)

    '''5. Fill the Lane Area'''
    # 추출 영역 색 채우기
    mask_region = np.zeros_like(frame)
    vertices_line = np.array([[int(x_left),720],[int(x_cross),int(y_cross)],[int(x_right),720]], dtype=np.int32)
    img = cv.putText(frame, "In Line  = ", (34,100), font, 2, In_line_color, 1, cv.LINE_AA)
    if(No_line_sign == 0):
        if(Thres_flag != 0):
            In_line_color = Line_Dan
            img = cv.putText(frame, "Line Change", (200,100), font, 2, Line_Dan, 1, cv.LINE_AA)
            cv.fillPoly(mask_region, [vertices_line], (0,0,255))
        else:
            In_line_color = Line_safe
            img = cv.putText(frame, "Safe", (200,100), font, 2, Line_safe, 1, cv.LINE_AA)
            cv.fillPoly(mask_region, [vertices_line], (0,255,0))
    else :
        In_line_color = Line_Dan
        img = cv.putText(frame, "Danger", (200,100), font, 2, Line_Dan, 1, cv.LINE_AA)
        cv.fillPoly(mask_region, [vertices_line], (0,0,255))
    
    #Lane 이동 방향 화살표 그리기
    if Thres_flag == 0:
        prev_bias = round((Bias),2) #Lane 이동 전까지 Bias 값 저장
        img = cv.putText(frame, str(round((abs(Bias)),2)) + '%', (350, 50), font, 2, (0, 0, 0), 1, cv.LINE_AA)
        cv.line(frame, (int(center),720), (int(center),630), (255,255,0), 3, cv.LINE_AA)
    else :
        img = cv.putText(frame, str(abs(prev_bias))+'%', (350, 50), font, 2, (0, 0, 0), 1, cv.LINE_AA)
        if 0< prev_bias < Bias_Threshold - 1 :
            cv.arrowedLine(frame, (int(width/2),int(height/2)), (800, int(height/2)), (0,0,255), 3)
        elif prev_bias < 0 :
            cv.arrowedLine(frame, (int(width/2),int(height/2)), (480, int(height/2)), (0,0,255), 3)

    k = cv.waitKey(5) & 0xFF
    if k == 27:
        break

    # 알고리즘 종료 시점/ FPS 출력
    terminate_t = timeit.default_timer()    
    FPS = int(1./(terminate_t - start_t ))
    img = cv.putText(frame, "FPS  = ", (34, 150), font, 2, (0, 0, 0), 1, cv.LINE_AA)
    img = cv.putText(frame, str(FPS), (200, 150), font, 2, (0, 0, 0), 1, cv.LINE_AA)

    # addWeighted() 함수로 투명도 조절
    weighted_frame = cv.addWeighted(frame, 1, mask_region, 0.3, 0) 
    cv.imshow('weighted_frame',weighted_frame)
cv.destroyAllWindows()
cap.release()
```
